
# Clustering and classification

## Load the data
Load the data from the MASS package and explore the structure and dimensions
```{r}
library(dplyr)
library(ggplot2)
library(MASS)
data("Boston")
str(Boston)
dim(Boston)
```
The `Boston` dataset is a data frame with 506 rows and 14 columns. The columns present different housing values in the suburbs of Boston.  

## Graphical overview of data
```{r, fig.width=14, fig.height=10}
pairs(Boston)
summary(Boston)
```

There's both continuous and binary variables in the data.  Some of the varibles are highly correlated, either negatively, such as lower status of the population (`lstat`) and median value of owner-occupied homes in \$1000 (`medv`), or positively, such as full-value property-tax rate per \$10,000 (`tax`) and proportion of non-retail business acres per town (`indus`).  
However, they are not all normally distributed or have the same variance which are the assumptions for LDA. So we need to scale the variables.

## Standardization and categorize the crime rate
We scale the values to have a mean of 0 and standard deviation of 1. 
```{r}
boston_scaled <- scale(Boston)
boston_scaled <- as.data.frame(boston_scaled)
apply(boston_scaled,2, sd, na.rm=TRUE)
summary(boston_scaled)
```
Then we use the `quantile` function to break the crime rates to 4 bins and categorize the crime rates to 4 different categories. Then we remove the original crime rate variuable and substitute it with the new categorial variables.
```{r}
bins <- quantile(boston_scaled$crim)
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))
boston_scaled <- dplyr::select(boston_scaled, -crim)
boston_scaled <- data.frame(boston_scaled, crime)
```
After that we divide the data to `test` and `train` sets, with 80 % of the data going to `train` set. We also save the correct classes from the `test` set and remove the `crime` variable from it.
```{r}
n <- nrow(boston_scaled)
ind <- sample(n,  size = n * 0.8)
train <- boston_scaled[ind,]
test <- boston_scaled[-ind,]
```

## Linear discriminant analysis
```{r}
lda.fit <- lda(crime~., data=train)
classes <- as.numeric(train$crime)
plot(lda.fit, dimen=2, col=classes)
```

## Predictions from the LDA
First we save the correct classess in the test set and remove it from the data. Then we predict the crime rate in the test set using the model from ther train set and cross-tabulate it with the correct classes.
```{r}
correct_classes <- test$crime
test <- dplyr::select(test, -crime)
lda.pred <- predict(lda.fit, newdata=test)
table(correct = correct_classes, predicted = lda.pred$class)
```
The model predicted quite well the crime categories. Probably with fewer classes, e.g. 3, the result would have been even better.

## Euclidean distance and k-means clustering
```{r}
data("Boston")
boston_scaled <- scale(Boston)
boston_euk_dist <- dist(boston_scaled)
```

Then we can run the k-means clustering with 3 clusters and visualise the result with few relevant variables. 
```{r, fig.width=14, fig.height=10}
km <-kmeans(boston_scaled, centers = 3)
pairs(boston_scaled[,6:10], col = km$cluster)
```
It seems that 3 clusters might not be the best, so we can investigate the optimal number of clusters. 
```{r}
set.seed(123)

k_max <- 10
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})
qplot(x = 1:k_max, y = twcss, geom = 'line')
```
  
It seems that 2 is the most optimal number of clusters, so let's visualise that on the same variables. 
```{r, fig.width=14, fig.height=10}
km <-kmeans(boston_scaled, centers = 2)
pairs(boston_scaled[,6:10], col = km$cluster)
```
**Looks better.**
